{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wellslu/anaconda3/envs/CS263/lib/python3.9/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from pytorch.src.models import ResNet\n",
    "\n",
    "# load model\n",
    "model_path = \"pytorch/model/ResNet18.pth\"\n",
    "model = ResNet(18, 10)\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()\n",
    "\n",
    "# transfer to TorchScript\n",
    "example_input = torch.randn(1, 1, 28, 28)  # input shape: [batch_size, channel, height, width]\n",
    "traced_model = torch.jit.trace(model, example_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  original_name=ResNet\n",
       "  (conv1): Sequential(\n",
       "    original_name=Sequential\n",
       "    (0): Conv2d(original_name=Conv2d)\n",
       "    (1): BatchNorm2d(original_name=BatchNorm2d)\n",
       "    (2): ReLU(original_name=ReLU)\n",
       "    (3): MaxPool2d(original_name=MaxPool2d)\n",
       "  )\n",
       "  (stage1): Sequential(\n",
       "    original_name=Sequential\n",
       "    (0): ResidualBlock(\n",
       "      original_name=ResidualBlock\n",
       "      (CB1): ConvBN(\n",
       "        original_name=ConvBN\n",
       "        (0): Conv2d(original_name=Conv2d)\n",
       "        (1): BatchNorm2d(original_name=BatchNorm2d)\n",
       "      )\n",
       "      (CB2): ConvBN(\n",
       "        original_name=ConvBN\n",
       "        (0): Conv2d(original_name=Conv2d)\n",
       "        (1): BatchNorm2d(original_name=BatchNorm2d)\n",
       "      )\n",
       "    )\n",
       "    (1): ResidualBlock(\n",
       "      original_name=ResidualBlock\n",
       "      (CB1): ConvBN(\n",
       "        original_name=ConvBN\n",
       "        (0): Conv2d(original_name=Conv2d)\n",
       "        (1): BatchNorm2d(original_name=BatchNorm2d)\n",
       "      )\n",
       "      (CB2): ConvBN(\n",
       "        original_name=ConvBN\n",
       "        (0): Conv2d(original_name=Conv2d)\n",
       "        (1): BatchNorm2d(original_name=BatchNorm2d)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (stage2): Sequential(\n",
       "    original_name=Sequential\n",
       "    (0): ResidualBlock(\n",
       "      original_name=ResidualBlock\n",
       "      (CB1): ConvBN(\n",
       "        original_name=ConvBN\n",
       "        (0): Conv2d(original_name=Conv2d)\n",
       "        (1): BatchNorm2d(original_name=BatchNorm2d)\n",
       "      )\n",
       "      (CB2): ConvBN(\n",
       "        original_name=ConvBN\n",
       "        (0): Conv2d(original_name=Conv2d)\n",
       "        (1): BatchNorm2d(original_name=BatchNorm2d)\n",
       "      )\n",
       "      (shortcut): ConvBN(\n",
       "        original_name=ConvBN\n",
       "        (0): Conv2d(original_name=Conv2d)\n",
       "        (1): BatchNorm2d(original_name=BatchNorm2d)\n",
       "      )\n",
       "    )\n",
       "    (1): ResidualBlock(\n",
       "      original_name=ResidualBlock\n",
       "      (CB1): ConvBN(\n",
       "        original_name=ConvBN\n",
       "        (0): Conv2d(original_name=Conv2d)\n",
       "        (1): BatchNorm2d(original_name=BatchNorm2d)\n",
       "      )\n",
       "      (CB2): ConvBN(\n",
       "        original_name=ConvBN\n",
       "        (0): Conv2d(original_name=Conv2d)\n",
       "        (1): BatchNorm2d(original_name=BatchNorm2d)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (stage3): Sequential(\n",
       "    original_name=Sequential\n",
       "    (0): ResidualBlock(\n",
       "      original_name=ResidualBlock\n",
       "      (CB1): ConvBN(\n",
       "        original_name=ConvBN\n",
       "        (0): Conv2d(original_name=Conv2d)\n",
       "        (1): BatchNorm2d(original_name=BatchNorm2d)\n",
       "      )\n",
       "      (CB2): ConvBN(\n",
       "        original_name=ConvBN\n",
       "        (0): Conv2d(original_name=Conv2d)\n",
       "        (1): BatchNorm2d(original_name=BatchNorm2d)\n",
       "      )\n",
       "      (shortcut): ConvBN(\n",
       "        original_name=ConvBN\n",
       "        (0): Conv2d(original_name=Conv2d)\n",
       "        (1): BatchNorm2d(original_name=BatchNorm2d)\n",
       "      )\n",
       "    )\n",
       "    (1): ResidualBlock(\n",
       "      original_name=ResidualBlock\n",
       "      (CB1): ConvBN(\n",
       "        original_name=ConvBN\n",
       "        (0): Conv2d(original_name=Conv2d)\n",
       "        (1): BatchNorm2d(original_name=BatchNorm2d)\n",
       "      )\n",
       "      (CB2): ConvBN(\n",
       "        original_name=ConvBN\n",
       "        (0): Conv2d(original_name=Conv2d)\n",
       "        (1): BatchNorm2d(original_name=BatchNorm2d)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (stage4): Sequential(\n",
       "    original_name=Sequential\n",
       "    (0): ResidualBlock(\n",
       "      original_name=ResidualBlock\n",
       "      (CB1): ConvBN(\n",
       "        original_name=ConvBN\n",
       "        (0): Conv2d(original_name=Conv2d)\n",
       "        (1): BatchNorm2d(original_name=BatchNorm2d)\n",
       "      )\n",
       "      (CB2): ConvBN(\n",
       "        original_name=ConvBN\n",
       "        (0): Conv2d(original_name=Conv2d)\n",
       "        (1): BatchNorm2d(original_name=BatchNorm2d)\n",
       "      )\n",
       "      (shortcut): ConvBN(\n",
       "        original_name=ConvBN\n",
       "        (0): Conv2d(original_name=Conv2d)\n",
       "        (1): BatchNorm2d(original_name=BatchNorm2d)\n",
       "      )\n",
       "    )\n",
       "    (1): ResidualBlock(\n",
       "      original_name=ResidualBlock\n",
       "      (CB1): ConvBN(\n",
       "        original_name=ConvBN\n",
       "        (0): Conv2d(original_name=Conv2d)\n",
       "        (1): BatchNorm2d(original_name=BatchNorm2d)\n",
       "      )\n",
       "      (CB2): ConvBN(\n",
       "        original_name=ConvBN\n",
       "        (0): Conv2d(original_name=Conv2d)\n",
       "        (1): BatchNorm2d(original_name=BatchNorm2d)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pool): AdaptiveAvgPool2d(original_name=AdaptiveAvgPool2d)\n",
       "  (classifier): Sequential(\n",
       "    original_name=Sequential\n",
       "    (0): Linear(original_name=Linear)\n",
       "    (1): Softmax(original_name=Softmax)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traced_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tvm\n",
    "from tvm import relay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"llvm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = example_input.shape\n",
    "input_name = \"input\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod, params = relay.frontend.from_pytorch(traced_model, [(input_name, input_shape)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type List[A] {\n",
      "  Cons(A, List[A]),\n",
      "  Nil,\n",
      "}\n",
      "\n",
      "type Option[A] {\n",
      "  Some(A),\n",
      "  None,\n",
      "}\n",
      "\n",
      "type Tree[A] {\n",
      "  Rose(A, List[Tree[A]]),\n",
      "}\n",
      "\n",
      "type tensor_float16_t {\n",
      "  tensor_nil_float16,\n",
      "  tensor0_float16(float16),\n",
      "  tensor1_float16(Tensor[(?), float16]),\n",
      "  tensor2_float16(Tensor[(?, ?), float16]),\n",
      "  tensor3_float16(Tensor[(?, ?, ?), float16]),\n",
      "  tensor4_float16(Tensor[(?, ?, ?, ?), float16]),\n",
      "  tensor5_float16(Tensor[(?, ?, ?, ?, ?), float16]),\n",
      "  tensor6_float16(Tensor[(?, ?, ?, ?, ?, ?), float16]),\n",
      "}\n",
      "\n",
      "type tensor_float32_t {\n",
      "  tensor_nil_float32,\n",
      "  tensor0_float32(float32),\n",
      "  tensor1_float32(Tensor[(?), float32]),\n",
      "  tensor2_float32(Tensor[(?, ?), float32]),\n",
      "  tensor3_float32(Tensor[(?, ?, ?), float32]),\n",
      "  tensor4_float32(Tensor[(?, ?, ?, ?), float32]),\n",
      "  tensor5_float32(Tensor[(?, ?, ?, ?, ?), float32]),\n",
      "  tensor6_float32(Tensor[(?, ?, ?, ?, ?, ?), float32]),\n",
      "}\n",
      "\n",
      "type tensor_float64_t {\n",
      "  tensor_nil_float64,\n",
      "  tensor0_float64(float64),\n",
      "  tensor1_float64(Tensor[(?), float64]),\n",
      "  tensor2_float64(Tensor[(?, ?), float64]),\n",
      "  tensor3_float64(Tensor[(?, ?, ?), float64]),\n",
      "  tensor4_float64(Tensor[(?, ?, ?, ?), float64]),\n",
      "  tensor5_float64(Tensor[(?, ?, ?, ?, ?), float64]),\n",
      "  tensor6_float64(Tensor[(?, ?, ?, ?, ?, ?), float64]),\n",
      "}\n",
      "\n",
      "type tensor_int16_t {\n",
      "  tensor_nil_int16,\n",
      "  tensor0_int16(int16),\n",
      "  tensor1_int16(Tensor[(?), int16]),\n",
      "  tensor2_int16(Tensor[(?, ?), int16]),\n",
      "  tensor3_int16(Tensor[(?, ?, ?), int16]),\n",
      "  tensor4_int16(Tensor[(?, ?, ?, ?), int16]),\n",
      "  tensor5_int16(Tensor[(?, ?, ?, ?, ?), int16]),\n",
      "  tensor6_int16(Tensor[(?, ?, ?, ?, ?, ?), int16]),\n",
      "}\n",
      "\n",
      "type tensor_int32_t {\n",
      "  tensor_nil_int32,\n",
      "  tensor0_int32(int32),\n",
      "  tensor1_int32(Tensor[(?), int32]),\n",
      "  tensor2_int32(Tensor[(?, ?), int32]),\n",
      "  tensor3_int32(Tensor[(?, ?, ?), int32]),\n",
      "  tensor4_int32(Tensor[(?, ?, ?, ?), int32]),\n",
      "  tensor5_int32(Tensor[(?, ?, ?, ?, ?), int32]),\n",
      "  tensor6_int32(Tensor[(?, ?, ?, ?, ?, ?), int32]),\n",
      "}\n",
      "\n",
      "type tensor_int64_t {\n",
      "  tensor_nil_int64,\n",
      "  tensor0_int64(int64),\n",
      "  tensor1_int64(Tensor[(?), int64]),\n",
      "  tensor2_int64(Tensor[(?, ?), int64]),\n",
      "  tensor3_int64(Tensor[(?, ?, ?), int64]),\n",
      "  tensor4_int64(Tensor[(?, ?, ?, ?), int64]),\n",
      "  tensor5_int64(Tensor[(?, ?, ?, ?, ?), int64]),\n",
      "  tensor6_int64(Tensor[(?, ?, ?, ?, ?, ?), int64]),\n",
      "}\n",
      "\n",
      "type tensor_int8_t {\n",
      "  tensor_nil_int8,\n",
      "  tensor0_int8(int8),\n",
      "  tensor1_int8(Tensor[(?), int8]),\n",
      "  tensor2_int8(Tensor[(?, ?), int8]),\n",
      "  tensor3_int8(Tensor[(?, ?, ?), int8]),\n",
      "  tensor4_int8(Tensor[(?, ?, ?, ?), int8]),\n",
      "  tensor5_int8(Tensor[(?, ?, ?, ?, ?), int8]),\n",
      "  tensor6_int8(Tensor[(?, ?, ?, ?, ?, ?), int8]),\n",
      "}\n",
      "\n",
      "type tensor_uint16_t {\n",
      "  tensor_nil_uint16,\n",
      "  tensor0_uint16(uint16),\n",
      "  tensor1_uint16(Tensor[(?), uint16]),\n",
      "  tensor2_uint16(Tensor[(?, ?), uint16]),\n",
      "  tensor3_uint16(Tensor[(?, ?, ?), uint16]),\n",
      "  tensor4_uint16(Tensor[(?, ?, ?, ?), uint16]),\n",
      "  tensor5_uint16(Tensor[(?, ?, ?, ?, ?), uint16]),\n",
      "  tensor6_uint16(Tensor[(?, ?, ?, ?, ?, ?), uint16]),\n",
      "}\n",
      "\n",
      "type tensor_uint8_t {\n",
      "  tensor_nil_uint8,\n",
      "  tensor0_uint8(uint8),\n",
      "  tensor1_uint8(Tensor[(?), uint8]),\n",
      "  tensor2_uint8(Tensor[(?, ?), uint8]),\n",
      "  tensor3_uint8(Tensor[(?, ?, ?), uint8]),\n",
      "  tensor4_uint8(Tensor[(?, ?, ?, ?), uint8]),\n",
      "  tensor5_uint8(Tensor[(?, ?, ?, ?, ?), uint8]),\n",
      "  tensor6_uint8(Tensor[(?, ?, ?, ?, ?, ?), uint8]),\n",
      "}\n",
      "\n",
      "def @main(%input: Tensor[(1, 1, 28, 28), float32] /* span=aten::_convolution_0.input:0:0 */, %aten::_convolution_0.weight: Tensor[(64, 1, 7, 7), float32] /* span=aten::_convolution_0.weight:0:0 */, %aten::batch_norm_0.weight: Tensor[(64), float32] /* span=aten::batch_norm_0.weight:0:0 */, %aten::batch_norm_0.bias: Tensor[(64), float32] /* span=aten::batch_norm_0.bias:0:0 */, %aten::batch_norm_0.running_mean: Tensor[(64), float32] /* span=aten::batch_norm_0.running_mean:0:0 */, %aten::batch_norm_0.running_var: Tensor[(64), float32] /* span=aten::batch_norm_0.running_var:0:0 */, %aten::_convolution_1.weight: Tensor[(64, 64, 3, 3), float32] /* span=aten::_convolution_1.weight:0:0 */, %aten::batch_norm_1.weight: Tensor[(64), float32] /* span=aten::batch_norm_1.weight:0:0 */, %aten::batch_norm_1.bias: Tensor[(64), float32] /* span=aten::batch_norm_1.bias:0:0 */, %aten::batch_norm_1.running_mean: Tensor[(64), float32] /* span=aten::batch_norm_1.running_mean:0:0 */, %aten::batch_norm_1.running_var: Tensor[(64), float32] /* span=aten::batch_norm_1.running_var:0:0 */, %aten::_convolution_2.weight: Tensor[(64, 64, 3, 3), float32] /* span=aten::_convolution_2.weight:0:0 */, %aten::batch_norm_2.weight: Tensor[(64), float32] /* span=aten::batch_norm_2.weight:0:0 */, %aten::batch_norm_2.bias: Tensor[(64), float32] /* span=aten::batch_norm_2.bias:0:0 */, %aten::batch_norm_2.running_mean: Tensor[(64), float32] /* span=aten::batch_norm_2.running_mean:0:0 */, %aten::batch_norm_2.running_var: Tensor[(64), float32] /* span=aten::batch_norm_2.running_var:0:0 */, %aten::_convolution_3.weight: Tensor[(64, 64, 3, 3), float32] /* span=aten::_convolution_3.weight:0:0 */, %aten::batch_norm_3.weight: Tensor[(64), float32] /* span=aten::batch_norm_3.weight:0:0 */, %aten::batch_norm_3.bias: Tensor[(64), float32] /* span=aten::batch_norm_3.bias:0:0 */, %aten::batch_norm_3.running_mean: Tensor[(64), float32] /* span=aten::batch_norm_3.running_mean:0:0 */, %aten::batch_norm_3.running_var: Tensor[(64), float32] /* span=aten::batch_norm_3.running_var:0:0 */, %aten::_convolution_4.weight: Tensor[(64, 64, 3, 3), float32] /* span=aten::_convolution_4.weight:0:0 */, %aten::batch_norm_4.weight: Tensor[(64), float32] /* span=aten::batch_norm_4.weight:0:0 */, %aten::batch_norm_4.bias: Tensor[(64), float32] /* span=aten::batch_norm_4.bias:0:0 */, %aten::batch_norm_4.running_mean: Tensor[(64), float32] /* span=aten::batch_norm_4.running_mean:0:0 */, %aten::batch_norm_4.running_var: Tensor[(64), float32] /* span=aten::batch_norm_4.running_var:0:0 */, %aten::_convolution_5.weight: Tensor[(128, 64, 3, 3), float32] /* span=aten::_convolution_5.weight:0:0 */, %aten::batch_norm_5.weight: Tensor[(128), float32] /* span=aten::batch_norm_5.weight:0:0 */, %aten::batch_norm_5.bias: Tensor[(128), float32] /* span=aten::batch_norm_5.bias:0:0 */, %aten::batch_norm_5.running_mean: Tensor[(128), float32] /* span=aten::batch_norm_5.running_mean:0:0 */, %aten::batch_norm_5.running_var: Tensor[(128), float32] /* span=aten::batch_norm_5.running_var:0:0 */, %aten::_convolution_6.weight: Tensor[(128, 128, 3, 3), float32] /* span=aten::_convolution_6.weight:0:0 */, %aten::batch_norm_6.weight: Tensor[(128), float32] /* span=aten::batch_norm_6.weight:0:0 */, %aten::batch_norm_6.bias: Tensor[(128), float32] /* span=aten::batch_norm_6.bias:0:0 */, %aten::batch_norm_6.running_mean: Tensor[(128), float32] /* span=aten::batch_norm_6.running_mean:0:0 */, %aten::batch_norm_6.running_var: Tensor[(128), float32] /* span=aten::batch_norm_6.running_var:0:0 */, %aten::_convolution_7.weight: Tensor[(128, 64, 1, 1), float32] /* span=aten::_convolution_7.weight:0:0 */, %aten::batch_norm_7.weight: Tensor[(128), float32] /* span=aten::batch_norm_7.weight:0:0 */, %aten::batch_norm_7.bias: Tensor[(128), float32] /* span=aten::batch_norm_7.bias:0:0 */, %aten::batch_norm_7.running_mean: Tensor[(128), float32] /* span=aten::batch_norm_7.running_mean:0:0 */, %aten::batch_norm_7.running_var: Tensor[(128), float32] /* span=aten::batch_norm_7.running_var:0:0 */, %aten::_convolution_8.weight: Tensor[(128, 128, 3, 3), float32] /* span=aten::_convolution_8.weight:0:0 */, %aten::batch_norm_8.weight: Tensor[(128), float32] /* span=aten::batch_norm_8.weight:0:0 */, %aten::batch_norm_8.bias: Tensor[(128), float32] /* span=aten::batch_norm_8.bias:0:0 */, %aten::batch_norm_8.running_mean: Tensor[(128), float32] /* span=aten::batch_norm_8.running_mean:0:0 */, %aten::batch_norm_8.running_var: Tensor[(128), float32] /* span=aten::batch_norm_8.running_var:0:0 */, %aten::_convolution_9.weight: Tensor[(128, 128, 3, 3), float32] /* span=aten::_convolution_9.weight:0:0 */, %aten::batch_norm_9.weight: Tensor[(128), float32] /* span=aten::batch_norm_9.weight:0:0 */, %aten::batch_norm_9.bias: Tensor[(128), float32] /* span=aten::batch_norm_9.bias:0:0 */, %aten::batch_norm_9.running_mean: Tensor[(128), float32] /* span=aten::batch_norm_9.running_mean:0:0 */, %aten::batch_norm_9.running_var: Tensor[(128), float32] /* span=aten::batch_norm_9.running_var:0:0 */, %aten::_convolution_10.weight: Tensor[(256, 128, 3, 3), float32] /* span=aten::_convolution_10.weight:0:0 */, %aten::batch_norm_10.weight: Tensor[(256), float32] /* span=aten::batch_norm_10.weight:0:0 */, %aten::batch_norm_10.bias: Tensor[(256), float32] /* span=aten::batch_norm_10.bias:0:0 */, %aten::batch_norm_10.running_mean: Tensor[(256), float32] /* span=aten::batch_norm_10.running_mean:0:0 */, %aten::batch_norm_10.running_var: Tensor[(256), float32] /* span=aten::batch_norm_10.running_var:0:0 */, %aten::_convolution_11.weight: Tensor[(256, 256, 3, 3), float32] /* span=aten::_convolution_11.weight:0:0 */, %aten::batch_norm_11.weight: Tensor[(256), float32] /* span=aten::batch_norm_11.weight:0:0 */, %aten::batch_norm_11.bias: Tensor[(256), float32] /* span=aten::batch_norm_11.bias:0:0 */, %aten::batch_norm_11.running_mean: Tensor[(256), float32] /* span=aten::batch_norm_11.running_mean:0:0 */, %aten::batch_norm_11.running_var: Tensor[(256), float32] /* span=aten::batch_norm_11.running_var:0:0 */, %aten::_convolution_12.weight: Tensor[(256, 128, 1, 1), float32] /* span=aten::_convolution_12.weight:0:0 */, %aten::batch_norm_12.weight: Tensor[(256), float32] /* span=aten::batch_norm_12.weight:0:0 */, %aten::batch_norm_12.bias: Tensor[(256), float32] /* span=aten::batch_norm_12.bias:0:0 */, %aten::batch_norm_12.running_mean: Tensor[(256), float32] /* span=aten::batch_norm_12.running_mean:0:0 */, %aten::batch_norm_12.running_var: Tensor[(256), float32] /* span=aten::batch_norm_12.running_var:0:0 */, %aten::_convolution_13.weight: Tensor[(256, 256, 3, 3), float32] /* span=aten::_convolution_13.weight:0:0 */, %aten::batch_norm_13.weight: Tensor[(256), float32] /* span=aten::batch_norm_13.weight:0:0 */, %aten::batch_norm_13.bias: Tensor[(256), float32] /* span=aten::batch_norm_13.bias:0:0 */, %aten::batch_norm_13.running_mean: Tensor[(256), float32] /* span=aten::batch_norm_13.running_mean:0:0 */, %aten::batch_norm_13.running_var: Tensor[(256), float32] /* span=aten::batch_norm_13.running_var:0:0 */, %aten::_convolution_14.weight: Tensor[(256, 256, 3, 3), float32] /* span=aten::_convolution_14.weight:0:0 */, %aten::batch_norm_14.weight: Tensor[(256), float32] /* span=aten::batch_norm_14.weight:0:0 */, %aten::batch_norm_14.bias: Tensor[(256), float32] /* span=aten::batch_norm_14.bias:0:0 */, %aten::batch_norm_14.running_mean: Tensor[(256), float32] /* span=aten::batch_norm_14.running_mean:0:0 */, %aten::batch_norm_14.running_var: Tensor[(256), float32] /* span=aten::batch_norm_14.running_var:0:0 */, %aten::_convolution_15.weight: Tensor[(512, 256, 3, 3), float32] /* span=aten::_convolution_15.weight:0:0 */, %aten::batch_norm_15.weight: Tensor[(512), float32] /* span=aten::batch_norm_15.weight:0:0 */, %aten::batch_norm_15.bias: Tensor[(512), float32] /* span=aten::batch_norm_15.bias:0:0 */, %aten::batch_norm_15.running_mean: Tensor[(512), float32] /* span=aten::batch_norm_15.running_mean:0:0 */, %aten::batch_norm_15.running_var: Tensor[(512), float32] /* span=aten::batch_norm_15.running_var:0:0 */, %aten::_convolution_16.weight: Tensor[(512, 512, 3, 3), float32] /* span=aten::_convolution_16.weight:0:0 */, %aten::batch_norm_16.weight: Tensor[(512), float32] /* span=aten::batch_norm_16.weight:0:0 */, %aten::batch_norm_16.bias: Tensor[(512), float32] /* span=aten::batch_norm_16.bias:0:0 */, %aten::batch_norm_16.running_mean: Tensor[(512), float32] /* span=aten::batch_norm_16.running_mean:0:0 */, %aten::batch_norm_16.running_var: Tensor[(512), float32] /* span=aten::batch_norm_16.running_var:0:0 */, %aten::_convolution_17.weight: Tensor[(512, 256, 1, 1), float32] /* span=aten::_convolution_17.weight:0:0 */, %aten::batch_norm_17.weight: Tensor[(512), float32] /* span=aten::batch_norm_17.weight:0:0 */, %aten::batch_norm_17.bias: Tensor[(512), float32] /* span=aten::batch_norm_17.bias:0:0 */, %aten::batch_norm_17.running_mean: Tensor[(512), float32] /* span=aten::batch_norm_17.running_mean:0:0 */, %aten::batch_norm_17.running_var: Tensor[(512), float32] /* span=aten::batch_norm_17.running_var:0:0 */, %aten::_convolution_18.weight: Tensor[(512, 512, 3, 3), float32] /* span=aten::_convolution_18.weight:0:0 */, %aten::batch_norm_18.weight: Tensor[(512), float32] /* span=aten::batch_norm_18.weight:0:0 */, %aten::batch_norm_18.bias: Tensor[(512), float32] /* span=aten::batch_norm_18.bias:0:0 */, %aten::batch_norm_18.running_mean: Tensor[(512), float32] /* span=aten::batch_norm_18.running_mean:0:0 */, %aten::batch_norm_18.running_var: Tensor[(512), float32] /* span=aten::batch_norm_18.running_var:0:0 */, %aten::_convolution_19.weight: Tensor[(512, 512, 3, 3), float32] /* span=aten::_convolution_19.weight:0:0 */, %aten::batch_norm_19.weight: Tensor[(512), float32] /* span=aten::batch_norm_19.weight:0:0 */, %aten::batch_norm_19.bias: Tensor[(512), float32] /* span=aten::batch_norm_19.bias:0:0 */, %aten::batch_norm_19.running_mean: Tensor[(512), float32] /* span=aten::batch_norm_19.running_mean:0:0 */, %aten::batch_norm_19.running_var: Tensor[(512), float32] /* span=aten::batch_norm_19.running_var:0:0 */, %aten::linear_0.weight: Tensor[(10, 512), float32] /* span=aten::linear_0.weight:0:0 */, %aten::linear_0.bias: Tensor[(10), float32] /* span=aten::linear_0.bias:0:0 */) {\n",
      "  %0 = nn.conv2d(%input, %aten::_convolution_0.weight, strides=[2, 2], padding=[3, 3, 3, 3], channels=64, kernel_size=[7, 7]) /* span=aten::_convolution_0:0:0 */;\n",
      "  %1 = nn.batch_norm(%0, %aten::batch_norm_0.weight, %aten::batch_norm_0.bias, %aten::batch_norm_0.running_mean, %aten::batch_norm_0.running_var) /* span=aten::batch_norm_0:0:0 */;\n",
      "  %2 = %1.0 /* span=aten::batch_norm_0:0:0 */;\n",
      "  %3 = nn.relu(%2) /* span=aten::relu__0:0:0 */;\n",
      "  %4 = nn.max_pool2d(%3, pool_size=[3, 3], strides=[2, 2], padding=[1, 1, 1, 1]) /* span=aten::max_pool2d_0:0:0 */;\n",
      "  %5 = nn.conv2d(%4, %aten::_convolution_1.weight, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* span=aten::_convolution_1:0:0 */;\n",
      "  %6 = nn.batch_norm(%5, %aten::batch_norm_1.weight, %aten::batch_norm_1.bias, %aten::batch_norm_1.running_mean, %aten::batch_norm_1.running_var) /* span=aten::batch_norm_1:0:0 */;\n",
      "  %7 = %6.0 /* span=aten::batch_norm_1:0:0 */;\n",
      "  %8 = nn.relu(%7) /* span=aten::relu_0:0:0 */;\n",
      "  %9 = nn.conv2d(%8, %aten::_convolution_2.weight, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* span=aten::_convolution_2:0:0 */;\n",
      "  %10 = nn.batch_norm(%9, %aten::batch_norm_2.weight, %aten::batch_norm_2.bias, %aten::batch_norm_2.running_mean, %aten::batch_norm_2.running_var) /* span=aten::batch_norm_2:0:0 */;\n",
      "  %11 = %10.0 /* span=aten::batch_norm_2:0:0 */;\n",
      "  %12 = add(%11, %4) /* span=aten::add__0:0:0 */;\n",
      "  %13 = nn.relu(%12) /* span=aten::relu_1:0:0 */;\n",
      "  %14 = nn.conv2d(%13, %aten::_convolution_3.weight, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* span=aten::_convolution_3:0:0 */;\n",
      "  %15 = nn.batch_norm(%14, %aten::batch_norm_3.weight, %aten::batch_norm_3.bias, %aten::batch_norm_3.running_mean, %aten::batch_norm_3.running_var) /* span=aten::batch_norm_3:0:0 */;\n",
      "  %16 = %15.0 /* span=aten::batch_norm_3:0:0 */;\n",
      "  %17 = nn.relu(%16) /* span=aten::relu_2:0:0 */;\n",
      "  %18 = nn.conv2d(%17, %aten::_convolution_4.weight, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* span=aten::_convolution_4:0:0 */;\n",
      "  %19 = nn.batch_norm(%18, %aten::batch_norm_4.weight, %aten::batch_norm_4.bias, %aten::batch_norm_4.running_mean, %aten::batch_norm_4.running_var) /* span=aten::batch_norm_4:0:0 */;\n",
      "  %20 = %19.0 /* span=aten::batch_norm_4:0:0 */;\n",
      "  %21 = add(%20, %13) /* span=aten::add__1:0:0 */;\n",
      "  %22 = nn.relu(%21) /* span=aten::relu_3:0:0 */;\n",
      "  %23 = nn.conv2d(%22, %aten::_convolution_5.weight, strides=[2, 2], padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* span=aten::_convolution_5:0:0 */;\n",
      "  %24 = nn.batch_norm(%23, %aten::batch_norm_5.weight, %aten::batch_norm_5.bias, %aten::batch_norm_5.running_mean, %aten::batch_norm_5.running_var) /* span=aten::batch_norm_5:0:0 */;\n",
      "  %25 = %24.0 /* span=aten::batch_norm_5:0:0 */;\n",
      "  %26 = nn.relu(%25) /* span=aten::relu_4:0:0 */;\n",
      "  %27 = nn.conv2d(%26, %aten::_convolution_6.weight, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* span=aten::_convolution_6:0:0 */;\n",
      "  %28 = nn.batch_norm(%27, %aten::batch_norm_6.weight, %aten::batch_norm_6.bias, %aten::batch_norm_6.running_mean, %aten::batch_norm_6.running_var) /* span=aten::batch_norm_6:0:0 */;\n",
      "  %29 = nn.conv2d(%22, %aten::_convolution_7.weight, strides=[2, 2], padding=[0, 0, 0, 0], channels=128, kernel_size=[1, 1]) /* span=aten::_convolution_7:0:0 */;\n",
      "  %30 = nn.batch_norm(%29, %aten::batch_norm_7.weight, %aten::batch_norm_7.bias, %aten::batch_norm_7.running_mean, %aten::batch_norm_7.running_var) /* span=aten::batch_norm_7:0:0 */;\n",
      "  %31 = %28.0 /* span=aten::batch_norm_6:0:0 */;\n",
      "  %32 = %30.0 /* span=aten::batch_norm_7:0:0 */;\n",
      "  %33 = add(%31, %32) /* span=aten::add__2:0:0 */;\n",
      "  %34 = nn.relu(%33) /* span=aten::relu_5:0:0 */;\n",
      "  %35 = nn.conv2d(%34, %aten::_convolution_8.weight, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* span=aten::_convolution_8:0:0 */;\n",
      "  %36 = nn.batch_norm(%35, %aten::batch_norm_8.weight, %aten::batch_norm_8.bias, %aten::batch_norm_8.running_mean, %aten::batch_norm_8.running_var) /* span=aten::batch_norm_8:0:0 */;\n",
      "  %37 = %36.0 /* span=aten::batch_norm_8:0:0 */;\n",
      "  %38 = nn.relu(%37) /* span=aten::relu_6:0:0 */;\n",
      "  %39 = nn.conv2d(%38, %aten::_convolution_9.weight, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* span=aten::_convolution_9:0:0 */;\n",
      "  %40 = nn.batch_norm(%39, %aten::batch_norm_9.weight, %aten::batch_norm_9.bias, %aten::batch_norm_9.running_mean, %aten::batch_norm_9.running_var) /* span=aten::batch_norm_9:0:0 */;\n",
      "  %41 = %40.0 /* span=aten::batch_norm_9:0:0 */;\n",
      "  %42 = add(%41, %34) /* span=aten::add__3:0:0 */;\n",
      "  %43 = nn.relu(%42) /* span=aten::relu_7:0:0 */;\n",
      "  %44 = nn.conv2d(%43, %aten::_convolution_10.weight, strides=[2, 2], padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* span=aten::_convolution_10:0:0 */;\n",
      "  %45 = nn.batch_norm(%44, %aten::batch_norm_10.weight, %aten::batch_norm_10.bias, %aten::batch_norm_10.running_mean, %aten::batch_norm_10.running_var) /* span=aten::batch_norm_10:0:0 */;\n",
      "  %46 = %45.0 /* span=aten::batch_norm_10:0:0 */;\n",
      "  %47 = nn.relu(%46) /* span=aten::relu_8:0:0 */;\n",
      "  %48 = nn.conv2d(%47, %aten::_convolution_11.weight, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* span=aten::_convolution_11:0:0 */;\n",
      "  %49 = nn.batch_norm(%48, %aten::batch_norm_11.weight, %aten::batch_norm_11.bias, %aten::batch_norm_11.running_mean, %aten::batch_norm_11.running_var) /* span=aten::batch_norm_11:0:0 */;\n",
      "  %50 = nn.conv2d(%43, %aten::_convolution_12.weight, strides=[2, 2], padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* span=aten::_convolution_12:0:0 */;\n",
      "  %51 = nn.batch_norm(%50, %aten::batch_norm_12.weight, %aten::batch_norm_12.bias, %aten::batch_norm_12.running_mean, %aten::batch_norm_12.running_var) /* span=aten::batch_norm_12:0:0 */;\n",
      "  %52 = %49.0 /* span=aten::batch_norm_11:0:0 */;\n",
      "  %53 = %51.0 /* span=aten::batch_norm_12:0:0 */;\n",
      "  %54 = add(%52, %53) /* span=aten::add__4:0:0 */;\n",
      "  %55 = nn.relu(%54) /* span=aten::relu_9:0:0 */;\n",
      "  %56 = nn.conv2d(%55, %aten::_convolution_13.weight, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* span=aten::_convolution_13:0:0 */;\n",
      "  %57 = nn.batch_norm(%56, %aten::batch_norm_13.weight, %aten::batch_norm_13.bias, %aten::batch_norm_13.running_mean, %aten::batch_norm_13.running_var) /* span=aten::batch_norm_13:0:0 */;\n",
      "  %58 = %57.0 /* span=aten::batch_norm_13:0:0 */;\n",
      "  %59 = nn.relu(%58) /* span=aten::relu_10:0:0 */;\n",
      "  %60 = nn.conv2d(%59, %aten::_convolution_14.weight, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* span=aten::_convolution_14:0:0 */;\n",
      "  %61 = nn.batch_norm(%60, %aten::batch_norm_14.weight, %aten::batch_norm_14.bias, %aten::batch_norm_14.running_mean, %aten::batch_norm_14.running_var) /* span=aten::batch_norm_14:0:0 */;\n",
      "  %62 = %61.0 /* span=aten::batch_norm_14:0:0 */;\n",
      "  %63 = add(%62, %55) /* span=aten::add__5:0:0 */;\n",
      "  %64 = nn.relu(%63) /* span=aten::relu_11:0:0 */;\n",
      "  %65 = nn.conv2d(%64, %aten::_convolution_15.weight, strides=[2, 2], padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* span=aten::_convolution_15:0:0 */;\n",
      "  %66 = nn.batch_norm(%65, %aten::batch_norm_15.weight, %aten::batch_norm_15.bias, %aten::batch_norm_15.running_mean, %aten::batch_norm_15.running_var) /* span=aten::batch_norm_15:0:0 */;\n",
      "  %67 = %66.0 /* span=aten::batch_norm_15:0:0 */;\n",
      "  %68 = nn.relu(%67) /* span=aten::relu_12:0:0 */;\n",
      "  %69 = nn.conv2d(%68, %aten::_convolution_16.weight, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* span=aten::_convolution_16:0:0 */;\n",
      "  %70 = nn.batch_norm(%69, %aten::batch_norm_16.weight, %aten::batch_norm_16.bias, %aten::batch_norm_16.running_mean, %aten::batch_norm_16.running_var) /* span=aten::batch_norm_16:0:0 */;\n",
      "  %71 = nn.conv2d(%64, %aten::_convolution_17.weight, strides=[2, 2], padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1]) /* span=aten::_convolution_17:0:0 */;\n",
      "  %72 = nn.batch_norm(%71, %aten::batch_norm_17.weight, %aten::batch_norm_17.bias, %aten::batch_norm_17.running_mean, %aten::batch_norm_17.running_var) /* span=aten::batch_norm_17:0:0 */;\n",
      "  %73 = %70.0 /* span=aten::batch_norm_16:0:0 */;\n",
      "  %74 = %72.0 /* span=aten::batch_norm_17:0:0 */;\n",
      "  %75 = add(%73, %74) /* span=aten::add__6:0:0 */;\n",
      "  %76 = nn.relu(%75) /* span=aten::relu_13:0:0 */;\n",
      "  %77 = nn.conv2d(%76, %aten::_convolution_18.weight, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* span=aten::_convolution_18:0:0 */;\n",
      "  %78 = nn.batch_norm(%77, %aten::batch_norm_18.weight, %aten::batch_norm_18.bias, %aten::batch_norm_18.running_mean, %aten::batch_norm_18.running_var) /* span=aten::batch_norm_18:0:0 */;\n",
      "  %79 = %78.0 /* span=aten::batch_norm_18:0:0 */;\n",
      "  %80 = nn.relu(%79) /* span=aten::relu_14:0:0 */;\n",
      "  %81 = nn.conv2d(%80, %aten::_convolution_19.weight, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* span=aten::_convolution_19:0:0 */;\n",
      "  %82 = nn.batch_norm(%81, %aten::batch_norm_19.weight, %aten::batch_norm_19.bias, %aten::batch_norm_19.running_mean, %aten::batch_norm_19.running_var) /* span=aten::batch_norm_19:0:0 */;\n",
      "  %83 = %82.0 /* span=aten::batch_norm_19:0:0 */;\n",
      "  %84 = add(%83, %76) /* span=aten::add__7:0:0 */;\n",
      "  %85 = nn.relu(%84) /* span=aten::relu_15:0:0 */;\n",
      "  %86 = nn.adaptive_avg_pool2d(%85, output_size=[1, 1]) /* span=aten::adaptive_avg_pool2d_0:0:0 */;\n",
      "  %87 = reshape(%86, newshape=[1, -1]) /* span=aten::view_0:0:0 */;\n",
      "  %88 = nn.dense(%87, %aten::linear_0.weight, units=None) /* span=aten::linear_0:0:0 */;\n",
      "  %89 = nn.bias_add(%88, %aten::linear_0.bias, axis=-1) /* span=aten::linear_0:0:0 */;\n",
      "  nn.softmax(%89, axis=1) /* span=aten::softmax_0:0:0 */\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[16:01:42] /home/wellslu/TVMvsCpp/tvm/src/runtime/threading_backend.cc:346: Warning: more than two frequencies detected! Forced big_count_ to 16\n",
      "One or more operators have not been tuned. Please tune your model for better performance. Use DEBUG logging level to see more details.\n"
     ]
    }
   ],
   "source": [
    "with tvm.transform.PassContext(opt_level=3):\n",
    "    lib = relay.build(mod, target=target, params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "from torchvision import datasets, transforms\n",
    "class MNIST(data.DataLoader):\n",
    "\n",
    "    def __init__(self, batch_size: int, train: bool, **kwargs):\n",
    "        transform = transforms.Compose([\n",
    "            # transforms.Resize((28,28)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "\n",
    "        dataset = datasets.MNIST('./pytorch/data/mnist', train=False, transform=transform)\n",
    "\n",
    "        super(MNIST, self).__init__(dataset=dataset, batch_size=batch_size, shuffle=train, **kwargs)\n",
    "\n",
    "test_loader = MNIST(batch_size=1, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (data, target) in enumerate(test_loader):\n",
    "    if i == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TVM running time: 1.0734262466430664\n",
      "TVM output: [[3.1008712e-23 1.2309332e-20 9.8386543e-24 1.6017456e-19 2.9745590e-22\n",
      "  8.2686773e-26 9.5610522e-23 1.0000000e+00 2.6356636e-23 2.4026528e-19]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tvm.contrib import graph_executor\n",
    "\n",
    "# create a runtime executor module\n",
    "dev = tvm.cpu()\n",
    "module = graph_executor.GraphModule(lib[\"default\"](dev))\n",
    "\n",
    "# input data\n",
    "input_data = data.reshape(1, 1, 28, 28)\n",
    "input_data = np.array(input_data.numpy(), dtype=\"float32\")\n",
    "\n",
    "# process model\n",
    "start = time.time()\n",
    "for i in range(1000):\n",
    "    tvm_data = tvm.nd.array(input_data)\n",
    "    module.set_input(input_name, tvm_data)\n",
    "    module.run()\n",
    "    output = module.get_output(0).asnumpy()\n",
    "end = time.time()\n",
    "print(\"TVM running time:\", end - start)\n",
    "\n",
    "# get output\n",
    "output = module.get_output(0).asnumpy()\n",
    "print(\"TVM output:\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch runnung time: 1.7855355739593506\n",
      "PyTorch output: [[3.1008592e-23 1.2309191e-20 9.8385044e-24 1.6017272e-19 2.9745476e-22\n",
      "  8.2685510e-26 9.5609797e-23 1.0000000e+00 2.6356335e-23 2.4026345e-19]]\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "start = time.time()\n",
    "for i in range(1000):\n",
    "    output = model(data)\n",
    "end = time.time()\n",
    "print(\"PyTorch runnung time:\", end - start)\n",
    "print(\"PyTorch output:\", output.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CS263",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
